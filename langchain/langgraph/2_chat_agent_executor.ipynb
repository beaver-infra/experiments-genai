{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf2404f-60a0-4607-9479-049eedddae0f",
   "metadata": {},
   "source": [
    "# Chat Agent Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92116fb-9654-4c25-9927-e0ec7451d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quite -U langchain langchain_community tavily-python dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b6f6d-18b7-4382-91a0-97dfca476558",
   "metadata": {},
   "source": [
    "## Create Langchain agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bec8359f-4c2c-4458-aa2b-7e267c737073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "from langchain_core.runnables import RunnableConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66d4f085-baa9-4129-b7e0-b844077a4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "TAVILY_API_KEY = os.environ.get(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b052812-1574-48c4-8764-91d9a4e1a04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose LLM\n",
    "llm = ChatOllama(model=\"llama2\", streaming=True)\n",
    "\n",
    "# Tools\n",
    "tools = [TavilySearchResults(tavily_api_key=TAVILY_API_KEY, max_results=1)]\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9915bb4c-83ab-4820-9816-a405583813f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent_runnable = create_react_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55eb3c8-4ab2-4310-8605-f2be1eb64fca",
   "metadata": {},
   "source": [
    "## Define the graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f2e1e48-73b9-4f1f-8459-717e74434246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7c1522f-5b9f-4173-821a-1c3e39a2014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d99d3-6249-438d-95f9-d15434ea55c0",
   "metadata": {},
   "source": [
    "## Define the nodes\n",
    "\n",
    "Define few different nodes and edges in our graph. A node can be eithher a function or a runnable.\n",
    "\n",
    "Here we need 2 types of nodes:\n",
    "1. The agent: responsible for deciding what actions to take\n",
    "2. A function to invoke tools: if the agent decides to take an action, this node will then execute that action.\n",
    "\n",
    "Edges could be conditional based on the output of node.\n",
    "1. Conditional Edge: after the agent is called, we should either:\n",
    "   a. If the agent said to take an action, then the function to invoke toools should be called.\n",
    "   b. If the agent said that it was finisied, then it should finish.\n",
    "2. Normal Edge: after the tools are invoked, it should always go back to the agent to decide wwhat to do next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a77abb32-592b-41d0-8a92-a5fedd6254f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import FunctionMessage\n",
    "from langgraph.prebuilt import ToolInvocation, ToolExecutor\n",
    "import json\n",
    "\n",
    "# Helper class to execute tools\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "# Define logic that will be used to determine which conditional edge to go down\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if \"function_call\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "def call_model(state):\n",
    "    # Note - Logic like only pass the 5 most recent message to model etc, can be added here\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_tool(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"])\n",
    "    )\n",
    "\n",
    "    response = tool_executor.invoke(action)\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0800ae-c8ec-4f67-8260-892175e99df8",
   "metadata": {},
   "source": [
    "## Define the graph\n",
    "\n",
    "Put all together and define the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89a6edb5-de78-4f51-900a-77bab6f6e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c58094f-bd02-4069-8bec-c99c02576ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in SF'),\n",
       "  AIMessage(content=\"\\nThe current weather in San Francisco, California is:\\n\\n* Temperature: 62°F (17°C)\\n* Humidity: 60%\\n* Wind: NW at 5 mph\\n* Conditions: Mostly Cloudy\\n* Forecast: A mix of sun and cloud with a chance of light rain showers throughout the day. High near 64°F (18°C) and low around 52°F (11°C).\\n\\nPlease note that this information is subject to change and may not be up-to-date or accurate. It's always a good idea to check the latest weather forecast before planning your day.\", response_metadata={'model': 'llama2', 'created_at': '2024-04-20T18:10:42.318812Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'total_duration': 2995407875, 'load_duration': 3124416, 'prompt_eval_count': 11, 'prompt_eval_duration': 267480000, 'eval_count': 152, 'eval_duration': 2722552000}, id='run-782ea381-5c1a-4c26-a0e9-6ecdd4a4f0bf-0')]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in SF\")]}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c6fcd-2253-40e1-a503-3c6c7364bb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
